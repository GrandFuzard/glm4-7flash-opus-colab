{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Do not use run all if you want to then remove the last cell\n",
        "# ğŸŸ£GLM-4.7 Flash Ã— Claude Opus 4.5 XHigh Reasoning Distill\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  What is this?\n",
        "\n",
        "A **ready-to-run** Google Colab notebook that loads and chats with GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill from this [**Repository**](https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF) â€” This Model is a quantization of a finetune of a finetune of GLM 4.7 Flash.\n",
        "\n",
        "> **TL;DR** â€” fine tune of GLM 4.7 flash with finetuning using opus 4.5 xhigh dataset\n",
        "\n",
        "### ğŸ—ï¸ Architecture\n",
        "\n",
        "```\n",
        "GLM-4.7 Flash (MoE 64Ã—2.6B)\n",
        "    â†“ fine-tuned on\n",
        "Claude Opus 4.5 High Reasoning dataset (250 examples)\n",
        "    â†“ quantized to\n",
        "GGUF (Q4_K_M / IQ4_XS / Q3_K_M)\n",
        "    â†“ runs on\n",
        "llama.cpp via llama-cpp-python (CUDA + CPU)\n",
        "    â†“ served through\n",
        "Gradio Chat UI with streaming\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“‹ How to use\n",
        "\n",
        "1. **Run cells 1â†’4 in order** (takes ~5 min first time for download)\n",
        "2. **Cell 2** â€” pick your model quant from the dropdown depending on what you want.\n",
        "3. **Cell 3** â€” adjust GPU layers slider (start at 48, lower if OOM)\n",
        "4. **Cell 4** â€” launches Gradio chat, click the **public URL** to open or chat in output itself\n",
        "5. **Cell 5** â€” run this only if you want to use a different model without having to disconnect and delete runtime this to fully unload model before switching quants this saves time\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”§ Default Sampling Parameters\n",
        "\n",
        "*From the [model card](https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF):*\n",
        "\n",
        "| Setting | General use | Tool-calling |\n",
        "|---|---|---|\n",
        "| Temperature | `1.0` | `0.7` |\n",
        "| Top-P | `0.95` | `1.0` |\n",
        "| Min-P | `0.01` | `0.01` |\n",
        "| Repeat penalty | `1.0` (disabled) | `1.0` (disabled) |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’¾ VRAM Requirements\n",
        "\n",
        "| Quant | File size | 48 layers GPU | Recommended split (T4 15GB) |\n",
        "|---|---|---|---|\n",
        "| **IQ2_M to Q3_K_M** | model<15 GB | âœ… fits fully | `48` layers GPU |\n",
        "| **IQ4_XS to Q6_K** | 26Gb>model>15Gb | ğŸª“ needs split | Variable depending on which model youre using |\n",
        "| **Q8_0 to F16** | model>26 GB | âŒ Model cannot fit on T4 with 12GB ram | Switch to a higher tier |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“œ Credits\n",
        "\n",
        "- **Base model**: [GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash) by zai-org\n",
        "- **Fine-tune 1**: [Unsloth](https://huggingface.co/unsloth/GLM-4.7-Flash)\n",
        "- **Fine-tune 2**: [TeichAI](https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill) using Claude Opus 4.5 xhigh reasoning [**dataset**](https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x)\n",
        "- **GGUF quantization**: [**TeichAI**](https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF)\n",
        "- I built this notebook using AI I dont know how to code if you have any issues blame the AI or Let me know in some way so I can update it (I dont even know how to use github)\n",
        "I want to write the specific model name also but idk if thats allowed."
      ],
      "metadata": {
        "id": "1yTSHlryvha0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U5rVbzed_bqz"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# @title ğŸ“¦ CELL 1 â€” Install Dependencies\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "os.environ[\"PIP_NO_WARN_SCRIPT_LOCATION\"] = \"1\"\n",
        "\n",
        "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 2>&1 | grep -v \"already satisfied\" | tail -1\n",
        "!pip install huggingface_hub gradio Pillow psutil 2>&1 | grep -v \"already satisfied\" | tail -1\n",
        "\n",
        "# Suppress the \"restart runtime\" warning â€” not needed here\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*previously imported.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*restart.*\")\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"âœ… Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# @title ğŸ“¥ CELL 2 â€” Download GGUF Model\n",
        "# ============================================================\n",
        "\n",
        "#@markdown ### Choose your quant:\n",
        "MODEL_CHOICE = \"Q3_K_M ~ 14.4GB\" #@param [\"IQ2_M ~ 9.9GB\", \"IQ3_XS ~ 12.3GB\", \"Q3_K_S ~ 13GB\", \"IQ3_M ~ 13.2GB\", \"Q3_K_M ~ 14.4GB\", \"IQ4_XS ~ 16GB (43 GPU layers)\", \"IQ4_NL ~ 17GB\", \"Q4_K_M ~ 18.1GB\", \"Q5_K_M ~ 21.3GB\", \"Q6_K ~ 24.6GB (28 GPU layers)\", \"Q8_0 ~ 31.8GB\", \"BF16 ~ 59.9GB\", \"F16 ~ 59.9GB\"]\n",
        "#@markdown <sub>IQ2_M â†’ Q3_K_M fit fully on free T4 GPU | IQ4_XS â†’ Q6_K need GPU+CPU split which I have noticed gives around 6tps at the start  (Adjust the GPU layer Slider to split the model) | Q8_0+ need mmap (Disk retrevial) or more VRAM (Higher memory machines)</sub>\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "MODEL_REPO = \"TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF\"\n",
        "\n",
        "# Map selection to filename\n",
        "_model_map = {\n",
        "    \"IQ2_M ~ 9.9GB\":   \"glm-4.7-flash-claude-4.5-opus.iq2_m.gguf\",\n",
        "    \"IQ3_XS ~ 12.3GB\": \"glm-4.7-flash-claude-4.5-opus.iq3_xs.gguf\",\n",
        "    \"Q3_K_S ~ 13GB\":   \"glm-4.7-flash-claude-4.5-opus.q3_k_s.gguf\",\n",
        "    \"IQ3_M ~ 13.2GB\":  \"glm-4.7-flash-claude-4.5-opus.iq3_m.gguf\",\n",
        "    \"Q3_K_M ~ 14.4GB\": \"glm-4.7-flash-claude-4.5-opus.q3_k_m.gguf\",\n",
        "    \"IQ4_XS ~ 16GB (43 GPU layers)\":   \"glm-4.7-flash-claude-4.5-opus.iq4_xs.gguf\",\n",
        "    \"IQ4_NL ~ 17GB\":   \"glm-4.7-flash-claude-4.5-opus.iq4_nl.gguf\",\n",
        "    \"Q4_K_M ~ 18.1GB\": \"glm-4.7-flash-claude-4.5-opus.q4_k_m.gguf\",\n",
        "    \"Q5_K_M ~ 21.3GB\": \"glm-4.7-flash-claude-4.5-opus.q5_k_m.gguf\",\n",
        "    \"Q6_K ~ 24.6GB (28 GPU layers)\":   \"glm-4.7-flash-claude-4.5-opus.q6_k.gguf\",\n",
        "    \"Q8_0 ~ 31.8GB\":   \"glm-4.7-flash-claude-4.5-opus.q8_0.gguf\",\n",
        "    \"BF16 ~ 59.9GB\":   \"glm-4.7-flash-claude-4.5-opus.bf16.gguf\",\n",
        "    \"F16 ~ 59.9GB\":    \"glm-4.7-flash-claude-4.5-opus.f16.gguf\",\n",
        "}\n",
        "\n",
        "MODEL_FILE = _model_map[MODEL_CHOICE]\n",
        "SAVE_DIR   = \"/content/models\"\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Clean up old quants to free disk\n",
        "for old in os.listdir(SAVE_DIR):\n",
        "    if old.endswith(\".gguf\") and old != MODEL_FILE:\n",
        "        old_path = os.path.join(SAVE_DIR, old)\n",
        "        old_size = os.path.getsize(old_path) / 1e9\n",
        "        os.remove(old_path)\n",
        "        print(f\"ğŸ—‘ï¸ Removed: {old} ({old_size:.1f} GB)\")\n",
        "\n",
        "print(f\"â¬‡ï¸ Downloading {MODEL_FILE}...\")\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=MODEL_REPO,\n",
        "    filename=MODEL_FILE,\n",
        "    local_dir=SAVE_DIR,\n",
        "    token=False,\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "size_gb = os.path.getsize(model_path) / 1e9\n",
        "quant_tag = MODEL_CHOICE.split(\" ~\")[0]\n",
        "print(f\"âœ… Downloaded â€” {size_gb:.2f} GB ({quant_tag})\")\n",
        "print(f\"ğŸ“ {model_path}\")\n",
        "\n",
        "# Store for Cell 3\n",
        "LOADED_MODEL_PATH = model_path\n",
        "LOADED_QUANT_TAG = quant_tag"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TuM0_7EQJhP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uaVURs5jRPVf"
      },
      "outputs": [],
      "source": [
        "#@title Keep Alive for Mobile Users (Optional Cell) (I dont even know if this does anything My phone needs 0.001 second to remove page from mem)\n",
        "from IPython.display import Audio, display\n",
        "display(Audio(\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\", autoplay=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cGcUn1X7_glD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# @title ğŸ§  CELL 3 â€” Load Model\n",
        "# ============================================================\n",
        "\n",
        "#@markdown ### GPU layers (48 = all):\n",
        "GPU_LAYERS = 48 #@param {type:\"slider\", min:0, max:48, step:1}\n",
        "\n",
        "\n",
        "CONTEXT_LENGTH = 4096\n",
        "\n",
        "from llama_cpp import Llama\n",
        "from IPython.display import clear_output\n",
        "import subprocess, sys, io, psutil\n",
        "\n",
        "print(f\"ğŸ“Š Loading {LOADED_QUANT_TAG} with {GPU_LAYERS}/48 layers on GPU, ctx={CONTEXT_LENGTH}\\n\")\n",
        "\n",
        "# Show memory before\n",
        "ram_before = psutil.virtual_memory().used / 1e9\n",
        "try:\n",
        "    smi = subprocess.check_output(\n",
        "        [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,noheader,nounits\"],\n",
        "        text=True\n",
        "    ).strip()\n",
        "    print(f\"   VRAM free: {int(smi)} MiB | RAM used: {ram_before:.1f} GB\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(f\"   â³ This may take a minute...\\n\")\n",
        "\n",
        "_stderr = sys.stderr\n",
        "sys.stderr = io.StringIO()\n",
        "\n",
        "try:\n",
        "    llm = Llama(\n",
        "        model_path=LOADED_MODEL_PATH,\n",
        "        n_gpu_layers=GPU_LAYERS,\n",
        "        n_ctx=CONTEXT_LENGTH,\n",
        "        n_batch=384,\n",
        "        use_mmap=False,\n",
        "        use_mlock=False,\n",
        "        verbose=True,\n",
        "    )\n",
        "    load_ok = True\n",
        "except (ValueError, RuntimeError) as e:\n",
        "    sys.stderr = _stderr\n",
        "    load_ok = False\n",
        "    print(f\"âŒ Failed with {GPU_LAYERS} layers + ctx={CONTEXT_LENGTH} â€” OOM\")\n",
        "    print(f\"   Try reducing GPU_LAYERS or CONTEXT_LENGTH and re-run this cell\")\n",
        "    llm = None\n",
        "\n",
        "if load_ok:\n",
        "    sys.stderr = _stderr\n",
        "    clear_output()\n",
        "    ram_after = psutil.virtual_memory().used / 1e9\n",
        "    cpu_layers = 48 - GPU_LAYERS\n",
        "    try:\n",
        "        smi = subprocess.check_output(\n",
        "            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.free\",\n",
        "             \"--format=csv,noheader,nounits\"], text=True\n",
        "        ).strip().split(\", \")\n",
        "        print(f\"âœ… G4.7FÂ·OXH loaded ({LOADED_QUANT_TAG})\")\n",
        "        print(f\"   ğŸ® VRAM: {int(smi[0])} MiB used | {smi[1]} MiB free\")\n",
        "        print(f\"   ğŸ§  RAM:  {ram_after:.1f} GB (+{ram_after - ram_before:.1f} GB for CPU layers)\")\n",
        "        print(f\"   ğŸ“Š {GPU_LAYERS} layers GPU + {cpu_layers} layers CPU | ctx={CONTEXT_LENGTH}\")\n",
        "    except:\n",
        "        print(f\"âœ… G4.7FÂ·OXH loaded â€” {GPU_LAYERS} GPU + {cpu_layers} CPU | ctx={CONTEXT_LENGTH} ({LOADED_QUANT_TAG})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6El-4Xla_nwb"
      },
      "outputs": [],
      "source": [
        "# ==========================================================================================\n",
        "# @title ğŸ’¬ CELL 4 â€” Gradio Chat UI (Click the link in the output to open Gradio in new tab)\n",
        "# ==========================================================================================\n",
        "\n",
        "import gradio as gr\n",
        "import time, sys, io, re, json, uuid, os, gc\n",
        "from datetime import datetime\n",
        "import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
        "\n",
        "MODEL_LABEL = \"G4.7FÂ·OXH\"\n",
        "\n",
        "# â”€â”€ Default settings â”€â”€\n",
        "DEFAULT_SYSTEM_PROMPT = \"You are a helpful assistant.\" #@param {type:\"string\"}\n",
        "DEFAULT_MAX_TOKENS = 1024\n",
        "DEFAULT_TEMPERATURE = 1.0\n",
        "DEFAULT_TOP_P = 0.95\n",
        "DEFAULT_MIN_P = 0.01\n",
        "DEFAULT_REPEAT_PENALTY = 1.0\n",
        "DEFAULT_FONT_SIZE = 14\n",
        "DEFAULT_UI_SCALE = 100\n",
        "#@markdown To change more parameters use settings in gradio UI\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Thinking Extraction & Formatting\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def extract_thinking(text):\n",
        "    has_close = \"</think>\" in text\n",
        "    has_open = \"<think>\" in text\n",
        "    if has_close and not has_open:\n",
        "        parts = text.split(\"</think>\", 1)\n",
        "        thinking = parts[0].strip()\n",
        "        clean = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "        if thinking and clean:\n",
        "            return thinking, clean\n",
        "        elif thinking and not clean:\n",
        "            return None, thinking\n",
        "        else:\n",
        "            return None, text.strip()\n",
        "    if has_open and has_close:\n",
        "        m = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
        "        if m:\n",
        "            thinking = m.group(1).strip()\n",
        "            clean = (text[:m.start()] + text[m.end():]).strip()\n",
        "            if thinking and clean:\n",
        "                return thinking, clean\n",
        "    return None, text.strip()\n",
        "\n",
        "\n",
        "def format_streaming(full_text, token_count, elapsed):\n",
        "    tok_s = token_count / elapsed if elapsed > 0 else 0\n",
        "    stats = f\"\\n\\n<sub>â³ {token_count} tok Â· {tok_s:.1f} t/s</sub>\"\n",
        "    if \"</think>\" in full_text:\n",
        "        thinking, clean = extract_thinking(full_text)\n",
        "        display = \"\"\n",
        "        if thinking:\n",
        "            display += (\n",
        "                f\"<details><summary>ğŸ’­ <i>Thinking</i> ({len(thinking)} chars)\"\n",
        "                f\" â€” click to expand</summary>\\n\\n{thinking}\\n\\n</details>\\n\\n\"\n",
        "            )\n",
        "        display += clean + stats\n",
        "        return display\n",
        "    else:\n",
        "        if token_count < 3:\n",
        "            return f\"ğŸ’­ *Thinking...*\" + stats\n",
        "        else:\n",
        "            return f\"ğŸ’­ **Thinking...**\\n\\n{full_text}\" + stats\n",
        "\n",
        "\n",
        "def format_final(full_text, token_count, elapsed):\n",
        "    tok_s = token_count / elapsed if elapsed > 0 else 0\n",
        "    thinking, clean = extract_thinking(full_text)\n",
        "    display = \"\"\n",
        "    if thinking:\n",
        "        display += (\n",
        "            f\"<details><summary>ğŸ’­ <i>Thinking</i> ({len(thinking)} chars)\"\n",
        "            f\" â€” click to expand</summary>\\n\\n{thinking}\\n\\n</details>\\n\\n\"\n",
        "        )\n",
        "    display += clean\n",
        "    display += f\"\\n\\n<sub>ğŸ“Š {token_count} tok Â· {tok_s:.1f} t/s Â· {elapsed:.1f}s</sub>\"\n",
        "    return display\n",
        "\n",
        "\n",
        "def clean_history_text(bot_msg):\n",
        "    if not bot_msg:\n",
        "        return \"\"\n",
        "    raw = re.sub(r\"<details>.*?</details>\", \"\", bot_msg, flags=re.DOTALL)\n",
        "    raw = re.sub(r\"<sub>.*?</sub>\", \"\", raw, flags=re.DOTALL)\n",
        "    raw = re.sub(r\"ğŸ’­ \\*\\*?Thinking\\.\\.\\.?\\*\\*?.*?\\n\\n\", \"\", raw, flags=re.DOTALL)\n",
        "    return raw.strip()\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# File/Image Processing\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "SUPPORTED_TEXT_EXT = {\".txt\", \".md\", \".py\", \".js\", \".ts\", \".html\", \".css\",\n",
        "                     \".json\", \".csv\", \".xml\", \".yaml\", \".yml\", \".log\",\n",
        "                     \".sh\", \".bat\", \".sql\", \".c\", \".cpp\", \".h\", \".java\",\n",
        "                     \".rs\", \".go\", \".rb\", \".php\", \".r\", \".tex\", \".ini\",\n",
        "                     \".cfg\", \".toml\", \".env\"}\n",
        "\n",
        "SUPPORTED_IMG_EXT = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".webp\"}\n",
        "\n",
        "\n",
        "def process_uploaded_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return None, None, None\n",
        "    filepath = file_obj.name if hasattr(file_obj, 'name') else str(file_obj)\n",
        "    filename = os.path.basename(filepath)\n",
        "    ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "    if ext in SUPPORTED_TEXT_EXT:\n",
        "        try:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                content = f.read()\n",
        "            if len(content) > 15000:\n",
        "                content = content[:15000] + f\"\\n\\n... [truncated â€” file was {len(content)} chars]\"\n",
        "            return content, \"text\", filename\n",
        "        except:\n",
        "            return None, \"unsupported\", filename\n",
        "\n",
        "    if ext == \".csv\":\n",
        "        try:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                content = f.read()\n",
        "            if len(content) > 15000:\n",
        "                content = content[:15000] + f\"\\n\\n... [truncated]\"\n",
        "            return content, \"text\", filename\n",
        "        except:\n",
        "            return None, \"unsupported\", filename\n",
        "\n",
        "    if ext in SUPPORTED_IMG_EXT:\n",
        "        return filepath, \"image\", filename\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        try:\n",
        "            import subprocess\n",
        "            result = subprocess.run(\n",
        "                [\"pdftotext\", filepath, \"-\"],\n",
        "                capture_output=True, text=True, timeout=30\n",
        "            )\n",
        "            if result.returncode == 0 and result.stdout.strip():\n",
        "                content = result.stdout.strip()\n",
        "                if len(content) > 15000:\n",
        "                    content = content[:15000] + f\"\\n\\n... [truncated]\"\n",
        "                return content, \"text\", filename\n",
        "        except:\n",
        "            pass\n",
        "        return None, \"unsupported\", filename\n",
        "\n",
        "    return None, \"unsupported\", filename\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Chat Store Helpers\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def make_chat(name=None):\n",
        "    cid = str(uuid.uuid4())[:8]\n",
        "    data = {\n",
        "        \"name\": name or \"New Chat\",\n",
        "        \"history\": [],\n",
        "        \"created\": datetime.now().strftime(\"%m/%d %H:%M\"),\n",
        "    }\n",
        "    return cid, data\n",
        "\n",
        "\n",
        "def chat_label(data):\n",
        "    return f\"{data['name']}  Â·  {data['created']}\"\n",
        "\n",
        "\n",
        "def radio_choices(store, search=\"\"):\n",
        "    items = []\n",
        "    for cid, data in store.items():\n",
        "        if not search or search.lower() in data[\"name\"].lower():\n",
        "            items.append((chat_label(data), data[\"created\"], cid))\n",
        "    items.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in items]\n",
        "\n",
        "\n",
        "def label_to_id(store, label):\n",
        "    for cid, data in store.items():\n",
        "        if chat_label(data) == label:\n",
        "            return cid\n",
        "    return None\n",
        "\n",
        "\n",
        "_init_id, _init_data = make_chat()\n",
        "_init_store = {_init_id: _init_data}\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Sidebar Event Handlers\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def on_new_chat(store, active, history):\n",
        "    if active in store:\n",
        "        store[active][\"history\"] = list(history)\n",
        "    cid, data = make_chat()\n",
        "    n = sum(1 for d in store.values() if d[\"name\"].startswith(\"New Chat\"))\n",
        "    if n > 0:\n",
        "        data[\"name\"] = f\"New Chat {n + 1}\"\n",
        "    store[cid] = data\n",
        "    choices = radio_choices(store)\n",
        "    lbl = chat_label(data)\n",
        "    return store, cid, [], gr.update(choices=choices, value=lbl), \"\"\n",
        "\n",
        "\n",
        "def on_select_chat(selected, store, active, history):\n",
        "    if not selected:\n",
        "        return active, history, store, \"\"\n",
        "    if active in store:\n",
        "        store[active][\"history\"] = list(history)\n",
        "    new_id = label_to_id(store, selected)\n",
        "    if new_id and new_id in store:\n",
        "        return new_id, store[new_id][\"history\"], store, store[new_id][\"name\"]\n",
        "    return active, history, store, \"\"\n",
        "\n",
        "\n",
        "def on_search(query, store):\n",
        "    return gr.update(choices=radio_choices(store, query))\n",
        "\n",
        "\n",
        "def on_delete(selected, store, active, history):\n",
        "    if not selected:\n",
        "        return store, active, history, gr.update(), \"\"\n",
        "    del_id = label_to_id(store, selected)\n",
        "    if not del_id or del_id not in store:\n",
        "        return store, active, history, gr.update(), \"\"\n",
        "    was_active = (del_id == active)\n",
        "    if not was_active and active in store:\n",
        "        store[active][\"history\"] = list(history)\n",
        "    del store[del_id]\n",
        "    if not store:\n",
        "        cid, data = make_chat()\n",
        "        store[cid] = data\n",
        "        active = cid\n",
        "        history = []\n",
        "    elif was_active:\n",
        "        active = list(store.keys())[0]\n",
        "        history = store[active][\"history\"]\n",
        "    choices = radio_choices(store)\n",
        "    lbl = chat_label(store[active])\n",
        "    return store, active, history, gr.update(choices=choices, value=lbl), \"\"\n",
        "\n",
        "\n",
        "def on_rename(selected, new_name, store):\n",
        "    if not selected or not new_name.strip():\n",
        "        return store, gr.update(), \"\"\n",
        "    cid = label_to_id(store, selected)\n",
        "    if cid and cid in store:\n",
        "        store[cid][\"name\"] = new_name.strip()\n",
        "    choices = radio_choices(store)\n",
        "    new_lbl = chat_label(store[cid]) if cid else None\n",
        "    return store, gr.update(choices=choices, value=new_lbl), \"\"\n",
        "\n",
        "\n",
        "def on_download_txt(selected, store):\n",
        "    if not selected:\n",
        "        return gr.update(visible=False)\n",
        "    cid = label_to_id(store, selected)\n",
        "    if not cid or cid not in store:\n",
        "        return gr.update(visible=False)\n",
        "    chat = store[cid]\n",
        "    safe = re.sub(r'[^\\w\\s-]', '', chat['name']).strip().replace(' ', '_') or \"chat\"\n",
        "    path = f\"/tmp/{safe}_{cid}.txt\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Chat: {chat['name']}\\n\")\n",
        "        f.write(f\"Created: {chat['created']}\\n\")\n",
        "        f.write(f\"Model: {MODEL_LABEL}\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        for u, b in chat.get(\"history\", []):\n",
        "            f.write(f\"You:\\n{u}\\n\\n\")\n",
        "            f.write(f\"Assistant:\\n{clean_history_text(b)}\\n\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\\n\")\n",
        "    return gr.update(value=path, visible=True)\n",
        "\n",
        "\n",
        "def on_download_json(selected, store):\n",
        "    if not selected:\n",
        "        return gr.update(visible=False)\n",
        "    cid = label_to_id(store, selected)\n",
        "    if not cid or cid not in store:\n",
        "        return gr.update(visible=False)\n",
        "    chat = store[cid]\n",
        "    safe = re.sub(r'[^\\w\\s-]', '', chat['name']).strip().replace(' ', '_') or \"chat\"\n",
        "    path = f\"/tmp/{safe}_{cid}.json\"\n",
        "    msgs = []\n",
        "    for u, b in chat.get(\"history\", []):\n",
        "        msgs.append({\"role\": \"user\", \"content\": u})\n",
        "        msgs.append({\"role\": \"assistant\", \"content\": clean_history_text(b)})\n",
        "    export = {\n",
        "        \"name\": chat[\"name\"],\n",
        "        \"created\": chat[\"created\"],\n",
        "        \"model\": f\"{MODEL_LABEL} ({LOADED_QUANT_TAG})\",\n",
        "        \"messages\": msgs,\n",
        "    }\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(export, f, indent=2, ensure_ascii=False)\n",
        "    return gr.update(value=path, visible=True)\n",
        "\n",
        "\n",
        "def on_font_change(chat_size, scale):\n",
        "    s = int(chat_size)\n",
        "    sc = int(scale)\n",
        "    factor = sc / 100\n",
        "    return f\"\"\"<style>\n",
        "    #chatbot .message-wrap, #chatbot .message-wrap *,\n",
        "    #chatbot .prose, #chatbot .prose *,\n",
        "    #chatbot p, #chatbot li, #chatbot span {{\n",
        "        font-size: {s}px !important;\n",
        "        line-height: {round(s * 1.55)}px !important;\n",
        "    }}\n",
        "    .gradio-container button {{\n",
        "        font-size: {round(14 * factor)}px !important;\n",
        "        padding: {round(8 * factor)}px {round(16 * factor)}px !important;\n",
        "    }}\n",
        "    .gradio-container input,\n",
        "    .gradio-container textarea,\n",
        "    .gradio-container select {{\n",
        "        font-size: {round(14 * factor)}px !important;\n",
        "    }}\n",
        "    .gradio-container label,\n",
        "    .gradio-container .label-wrap,\n",
        "    .gradio-container .info {{\n",
        "        font-size: {round(12 * factor)}px !important;\n",
        "    }}\n",
        "    .gradio-container .markdown-text,\n",
        "    .gradio-container .markdown-text * {{\n",
        "        font-size: {round(14 * factor)}px !important;\n",
        "    }}\n",
        "    .gradio-container h2 {{\n",
        "        font-size: {round(24 * factor)}px !important;\n",
        "    }}\n",
        "    .gradio-container h3 {{\n",
        "        font-size: {round(18 * factor)}px !important;\n",
        "    }}\n",
        "    .gradio-container sub {{\n",
        "        font-size: {round(11 * factor)}px !important;\n",
        "    }}\n",
        "    #sidebar-col .wrap label {{\n",
        "        font-size: {round(13 * factor)}px !important;\n",
        "        padding: {round(8 * factor)}px {round(10 * factor)}px !important;\n",
        "    }}\n",
        "    #sidebar-rail button {{\n",
        "        width: {round(38 * factor)}px !important;\n",
        "        height: {round(38 * factor)}px !important;\n",
        "        font-size: {round(16 * factor)}px !important;\n",
        "    }}\n",
        "    #settings-btn, #upload-btn, #collapse-btn {{\n",
        "        min-width: {round(42 * factor)}px !important;\n",
        "        height: {round(36 * factor)}px !important;\n",
        "        font-size: {round(18 * factor)}px !important;\n",
        "    }}\n",
        "    </style>\"\"\"\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Context Reload\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def reload_context(new_ctx):\n",
        "    global llm, CONTEXT_LENGTH\n",
        "    new_ctx = int(new_ctx)\n",
        "\n",
        "    if new_ctx == CONTEXT_LENGTH:\n",
        "        return f\"â„¹ï¸ Already at ctx={new_ctx}\"\n",
        "\n",
        "    try:\n",
        "        llm.close()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        del llm\n",
        "    except:\n",
        "        pass\n",
        "    gc.collect()\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        torch.cuda.empty_cache()\n",
        "    except:\n",
        "        pass\n",
        "    gc.collect()\n",
        "\n",
        "    try:\n",
        "        from llama_cpp import Llama\n",
        "        llm = Llama(\n",
        "            model_path=LOADED_MODEL_PATH,\n",
        "            n_gpu_layers=GPU_LAYERS,\n",
        "            n_ctx=new_ctx,\n",
        "            n_batch=384,\n",
        "            use_mmap=False,\n",
        "            use_mlock=False,\n",
        "            verbose=False,\n",
        "        )\n",
        "        CONTEXT_LENGTH = new_ctx\n",
        "        return f\"âœ… Reloaded with ctx={new_ctx}\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Failed: {e}\\n\\nTry a smaller context or fewer GPU layers.\"\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Chat Function (Streaming + File Upload)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def chat_fn(user_message, file_upload, history, store, active,\n",
        "            system_prompt, max_tokens, temperature, top_p, min_p, repeat_penalty):\n",
        "\n",
        "    file_context = \"\"\n",
        "    file_notice = \"\"\n",
        "\n",
        "    if file_upload is not None:\n",
        "        content, ftype, fname = process_uploaded_file(file_upload)\n",
        "        if ftype == \"text\" and content:\n",
        "            file_context = f\"\\n\\n---\\nğŸ“ **Attached file: {fname}**\\n```\\n{content}\\n```\\n---\\n\"\n",
        "            file_notice = f\"ğŸ“ *{fname}*\\n\\n\"\n",
        "        elif ftype == \"image\":\n",
        "            file_context = (\n",
        "                f\"\\n\\n[User attached an image: {fname}. \"\n",
        "                f\"Note: This model is text-only and cannot process images. \"\n",
        "                f\"Please describe what you'd like help with regarding this image.]\"\n",
        "            )\n",
        "            file_notice = f\"ğŸ–¼ï¸ *{fname}* (âš ï¸ text-only model)\\n\\n\"\n",
        "        elif ftype == \"unsupported\":\n",
        "            file_context = f\"\\n\\n[User attached a file: {fname}. This file type is not supported for direct reading.]\"\n",
        "            file_notice = f\"ğŸ“„ *{fname}* (unsupported format)\\n\\n\"\n",
        "\n",
        "    combined_message = user_message.strip()\n",
        "    if not combined_message and not file_context:\n",
        "        yield \"\", None, history, store, gr.update()\n",
        "        return\n",
        "\n",
        "    if not combined_message and file_context:\n",
        "        combined_message = \"Please analyze the attached file.\"\n",
        "\n",
        "    full_user_message = combined_message + file_context\n",
        "    display_user_message = file_notice + combined_message\n",
        "\n",
        "    if active in store and store[active][\"name\"].startswith(\"New Chat\") and not history:\n",
        "        trunc = combined_message[:30].strip()\n",
        "        if len(combined_message) > 30:\n",
        "            trunc += \"â€¦\"\n",
        "        store[active][\"name\"] = trunc\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    for u, b in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": u})\n",
        "        if b:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": clean_history_text(b)})\n",
        "    messages.append({\"role\": \"user\", \"content\": full_user_message})\n",
        "\n",
        "    history = history + [(display_user_message, \"â³ ...\")]\n",
        "    yield \"\", None, history, store, gr.update()\n",
        "\n",
        "    t0 = time.time()\n",
        "    full_response = \"\"\n",
        "    token_count = 0\n",
        "\n",
        "    try:\n",
        "        stream = llm.create_chat_completion(\n",
        "            messages=messages,\n",
        "            max_tokens=int(max_tokens),\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            min_p=min_p,\n",
        "            repeat_penalty=repeat_penalty,\n",
        "            stream=True,\n",
        "        )\n",
        "        for chunk in stream:\n",
        "            delta = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\", \"\")\n",
        "            if not delta:\n",
        "                continue\n",
        "            full_response += delta\n",
        "            token_count += 1\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            # Yield every 3 tokens to reduce scroll jank\n",
        "            if token_count % 3 == 0 or \"\\n\" in delta:\n",
        "                display = format_streaming(full_response, token_count, elapsed)\n",
        "                history[-1] = (display_user_message, display)\n",
        "                yield \"\", None, history, store, gr.update()\n",
        "\n",
        "    except Exception:\n",
        "        prompt = \"\"\n",
        "        for m in messages:\n",
        "            prompt += f\"<|{m['role']}|>\\n{m['content']}<|endoftext|>\\n\"\n",
        "        prompt += \"<|assistant|>\\n\"\n",
        "        output = llm(\n",
        "            prompt,\n",
        "            max_tokens=int(max_tokens),\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            min_p=min_p,\n",
        "            repeat_penalty=repeat_penalty,\n",
        "            stop=[\"<|endoftext|>\", \"<|user|>\", \"<|end|>\"],\n",
        "        )\n",
        "        full_response = output[\"choices\"][0][\"text\"].strip()\n",
        "        token_count = len(full_response.split())\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    display = format_final(full_response, token_count, elapsed)\n",
        "    history[-1] = (display_user_message, display)\n",
        "\n",
        "    if active in store:\n",
        "        store[active][\"history\"] = list(history)\n",
        "    choices = radio_choices(store)\n",
        "    active_lbl = chat_label(store[active]) if active in store else None\n",
        "    yield \"\", None, history, store, gr.update(choices=choices, value=active_lbl)\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Avatars\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def make_avatar(letter, bg_color, text_color=\"white\", size=64):\n",
        "    img = PIL.Image.new(\"RGB\", (size, size), bg_color)\n",
        "    draw = PIL.ImageDraw.Draw(img)\n",
        "    try:\n",
        "        font = PIL.ImageFont.truetype(\n",
        "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", size // 2\n",
        "        )\n",
        "    except Exception:\n",
        "        font = PIL.ImageFont.load_default()\n",
        "    bbox = draw.textbbox((0, 0), letter, font=font)\n",
        "    tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
        "    draw.text(\n",
        "        ((size - tw) / 2, (size - th) / 2 - 2), letter,\n",
        "        fill=text_color, font=font,\n",
        "    )\n",
        "    path = f\"/tmp/avatar_{letter}.png\"\n",
        "    img.save(path)\n",
        "    return path\n",
        "\n",
        "user_avatar = make_avatar(\"Y\", \"#2563eb\")\n",
        "bot_avatar  = make_avatar(\"G\", \"#7c3aed\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Gradio Layout\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "theme = gr.themes.Soft(\n",
        "    primary_hue=\"violet\",\n",
        "    secondary_hue=\"indigo\",\n",
        "    font=gr.themes.GoogleFont(\"Inter\"),\n",
        ")\n",
        "\n",
        "CUSTOM_CSS = \"\"\"\n",
        "    #settings-btn, #upload-btn {\n",
        "        min-width: 42px !important;\n",
        "        padding: 8px !important;\n",
        "    }\n",
        "    #sidebar-col {\n",
        "        border-right: 1px solid #e5e7eb;\n",
        "        padding-right: 12px !important;\n",
        "        min-width: 240px;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "    #collapse-btn {\n",
        "        min-width: 36px !important;\n",
        "        max-width: 36px !important;\n",
        "        height: 36px !important;\n",
        "        padding: 0 !important;\n",
        "        border-radius: 8px !important;\n",
        "        font-size: 18px !important;\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        justify-content: center;\n",
        "    }\n",
        "    #collapse-btn:hover {\n",
        "        background: #ede9fe !important;\n",
        "    }\n",
        "    #sidebar-rail {\n",
        "        min-width: 48px !important;\n",
        "        max-width: 48px !important;\n",
        "        border-right: 1px solid #e5e7eb;\n",
        "        display: flex;\n",
        "        flex-direction: column;\n",
        "        align-items: center;\n",
        "        padding-top: 8px !important;\n",
        "        gap: 8px;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "    #sidebar-rail button {\n",
        "        width: 38px !important;\n",
        "        height: 38px !important;\n",
        "        min-width: 38px !important;\n",
        "        padding: 0 !important;\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        justify-content: center;\n",
        "        border-radius: 8px;\n",
        "        font-size: 16px;\n",
        "    }\n",
        "    #sidebar-rail button:hover {\n",
        "        background: #ede9fe !important;\n",
        "    }\n",
        "    #chatbot {\n",
        "        min-height: 480px;\n",
        "        overflow-y: auto !important;\n",
        "    }\n",
        "    #sidebar-col .wrap label {\n",
        "        border-radius: 8px !important;\n",
        "        padding: 8px 10px !important;\n",
        "        margin: 2px 0 !important;\n",
        "        font-size: 13px !important;\n",
        "        transition: background 0.15s;\n",
        "    }\n",
        "    #sidebar-col .wrap label:hover {\n",
        "        background: #f5f3ff !important;\n",
        "    }\n",
        "    #sidebar-col .wrap input[type=\"radio\"]:checked + label {\n",
        "        background: #ede9fe !important;\n",
        "        border-color: #a78bfa !important;\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "SCROLL_JS = \"\"\"\n",
        "<script>\n",
        "(() => {\n",
        "    let userScrolledUp = false;\n",
        "\n",
        "    function setup() {\n",
        "        const wrap = document.querySelector('#chatbot .wrap')\n",
        "                  || document.querySelector('#chatbot > div > div');\n",
        "        if (!wrap) return setTimeout(setup, 800);\n",
        "\n",
        "        wrap.addEventListener('scroll', () => {\n",
        "            const gap = wrap.scrollHeight - wrap.scrollTop - wrap.clientHeight;\n",
        "            userScrolledUp = gap > 100;\n",
        "        }, {passive: true});\n",
        "\n",
        "        const mo = new MutationObserver(() => {\n",
        "            if (!userScrolledUp) {\n",
        "                requestAnimationFrame(() => {\n",
        "                    wrap.scrollTop = wrap.scrollHeight;\n",
        "                });\n",
        "            }\n",
        "        });\n",
        "        mo.observe(wrap, {childList: true, subtree: true, characterData: true});\n",
        "\n",
        "        try {\n",
        "            const pd = Object.getOwnPropertyDescriptor(Element.prototype, 'scrollTop');\n",
        "            if (pd && pd.set) {\n",
        "                Object.defineProperty(wrap, 'scrollTop', {\n",
        "                    get() { return pd.get.call(this); },\n",
        "                    set(v) {\n",
        "                        if (userScrolledUp) return;\n",
        "                        pd.set.call(this, v);\n",
        "                    }\n",
        "                });\n",
        "            }\n",
        "        } catch(e) {}\n",
        "\n",
        "        document.addEventListener('click', e => {\n",
        "            const b = e.target.closest('button');\n",
        "            if (b && (b.textContent.includes('Send') || b.classList.contains('primary')))\n",
        "                userScrolledUp = false;\n",
        "        });\n",
        "        document.addEventListener('keydown', e => {\n",
        "            if (e.key==='Enter' && !e.shiftKey && e.target.tagName==='TEXTAREA')\n",
        "                userScrolledUp = false;\n",
        "        });\n",
        "    }\n",
        "    setTimeout(setup, 2000);\n",
        "})();\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=theme, title=MODEL_LABEL, css=CUSTOM_CSS) as demo:\n",
        "\n",
        "    # â”€â”€ State â”€â”€\n",
        "    chat_store     = gr.State(_init_store)\n",
        "    active_chat_id = gr.State(_init_id)\n",
        "    sidebar_state  = gr.State(True)\n",
        "    settings_state = gr.State(False)\n",
        "\n",
        "    font_css = gr.HTML(SCROLL_JS)\n",
        "\n",
        "    gr.Markdown(\n",
        "        f\"## ğŸŸ£ {MODEL_LABEL}\\n\"\n",
        "        f\"<sub>GLM-4.7 Flash Â· Claude Opus 4.5 Â· XHigh Reasoning Distill\"\n",
        "        f\" Â· {LOADED_QUANT_TAG} Â· ctx {CONTEXT_LENGTH}</sub>\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        # COLLAPSED SIDEBAR (thin rail)\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        with gr.Column(visible=False, elem_id=\"sidebar-rail\") as sidebar_rail:\n",
        "            rail_expand_btn  = gr.Button(\"â˜°\", size=\"sm\")\n",
        "            rail_newchat_btn = gr.Button(\"âœš\", size=\"sm\")\n",
        "\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        # EXPANDED SIDEBAR\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        with gr.Column(scale=1, visible=True, elem_id=\"sidebar-col\") as sidebar_panel:\n",
        "\n",
        "            with gr.Row():\n",
        "                gr.Markdown(\"**ğŸ’¬ Chats**\")\n",
        "                collapse_btn = gr.Button(\"âœ•\", elem_id=\"collapse-btn\", size=\"sm\", scale=0)\n",
        "\n",
        "            search_box = gr.Textbox(\n",
        "                placeholder=\"ğŸ” Search chatsâ€¦\",\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "            )\n",
        "\n",
        "            new_chat_btn = gr.Button(\"+ New Chat\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "            chat_list = gr.Radio(\n",
        "                choices=radio_choices(_init_store),\n",
        "                value=chat_label(_init_data),\n",
        "                label=\"\",\n",
        "                interactive=True,\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"**Chat Actions**\")\n",
        "\n",
        "            with gr.Row():\n",
        "                rename_input = gr.Textbox(\n",
        "                    placeholder=\"New nameâ€¦\",\n",
        "                    show_label=False,\n",
        "                    container=False,\n",
        "                    scale=3,\n",
        "                )\n",
        "                rename_btn = gr.Button(\"Rename\", size=\"sm\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                delete_btn  = gr.Button(\"ğŸ—‘ï¸ Delete\", size=\"sm\", variant=\"stop\")\n",
        "                dl_txt_btn  = gr.Button(\"ğŸ“¥ TXT\", size=\"sm\")\n",
        "                dl_json_btn = gr.Button(\"ğŸ“¥ JSON\", size=\"sm\")\n",
        "\n",
        "            download_output = gr.File(\n",
        "                label=\"Download ready\", visible=False, interactive=False\n",
        "            )\n",
        "\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        # MAIN CHAT AREA\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        with gr.Column(scale=4):\n",
        "\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=520,\n",
        "                show_label=False,\n",
        "                bubble_full_width=False,\n",
        "                avatar_images=(user_avatar, bot_avatar),\n",
        "                elem_id=\"chatbot\",\n",
        "            )\n",
        "\n",
        "            file_upload = gr.File(\n",
        "                label=\"ğŸ“ Attach a file (text, code, csv, pdf, image)\",\n",
        "                file_types=[\n",
        "                    \".txt\", \".md\", \".py\", \".js\", \".ts\", \".html\", \".css\",\n",
        "                    \".json\", \".csv\", \".xml\", \".yaml\", \".yml\", \".log\",\n",
        "                    \".sh\", \".sql\", \".c\", \".cpp\", \".h\", \".java\", \".rs\",\n",
        "                    \".go\", \".rb\", \".php\", \".r\", \".tex\", \".ini\", \".cfg\",\n",
        "                    \".toml\", \".env\", \".pdf\",\n",
        "                    \".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".webp\",\n",
        "                ],\n",
        "                visible=False,\n",
        "            )\n",
        "\n",
        "            upload_status = gr.Markdown(\"\", visible=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                upload_btn = gr.Button(\"ğŸ“\", elem_id=\"upload-btn\", scale=0)\n",
        "                txt = gr.Textbox(\n",
        "                    placeholder=\"Type your message hereâ€¦\",\n",
        "                    show_label=False,\n",
        "                    scale=8,\n",
        "                    container=False,\n",
        "                )\n",
        "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "                settings_toggle = gr.Button(\"âš™ï¸\", elem_id=\"settings-btn\", scale=0)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"ğŸ—‘ï¸ Clear Chat\", size=\"sm\")\n",
        "\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        # RIGHT SETTINGS PANEL\n",
        "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "        with gr.Column(scale=2, visible=False) as settings_panel:\n",
        "\n",
        "            gr.Markdown(\"### âš™ï¸ Settings\")\n",
        "\n",
        "            system_prompt = gr.Textbox(\n",
        "                label=\"System Prompt\",\n",
        "                value=DEFAULT_SYSTEM_PROMPT,\n",
        "                lines=3, max_lines=6,\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"**Sampling**\")\n",
        "\n",
        "            temperature = gr.Slider(\n",
        "                0.0, 2.0, DEFAULT_TEMPERATURE, step=0.05,\n",
        "                label=\"Temperature\",\n",
        "                info=\"Higher = creative Â· Lower = focused\",\n",
        "            )\n",
        "            max_tokens = gr.Slider(\n",
        "                64, 4096, DEFAULT_MAX_TOKENS, step=64,\n",
        "                label=\"Max Tokens\",\n",
        "                info=\"Maximum response length\",\n",
        "            )\n",
        "            top_p = gr.Slider(\n",
        "                0.0, 1.0, DEFAULT_TOP_P, step=0.01,\n",
        "                label=\"Top-P\",\n",
        "                info=\"Nucleus sampling\",\n",
        "            )\n",
        "            min_p = gr.Slider(\n",
        "                0.0, 0.5, DEFAULT_MIN_P, step=0.005,\n",
        "                label=\"Min-P\",\n",
        "                info=\"Use 0.01 for llama.cpp\",\n",
        "            )\n",
        "            repeat_penalty = gr.Slider(\n",
        "                1.0, 2.0, DEFAULT_REPEAT_PENALTY, step=0.05,\n",
        "                label=\"Repeat Penalty\",\n",
        "                info=\"1.0 = disabled\",\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"**Model**\")\n",
        "\n",
        "            ctx_dropdown = gr.Dropdown(\n",
        "                choices=[2048, 4096, 8192, 16384, 32768, 65536],\n",
        "                value=CONTEXT_LENGTH,\n",
        "                label=\"Context Length\",\n",
        "                info=\"Max conversation memory (model supports up to 200K)\",\n",
        "            )\n",
        "\n",
        "            ctx_reload_btn = gr.Button(\"ğŸ”„ Apply Context (reloads model)\", size=\"sm\")\n",
        "            ctx_status = gr.Markdown(\"\")\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"**Display**\")\n",
        "\n",
        "            font_size = gr.Slider(\n",
        "                10, 24, DEFAULT_FONT_SIZE, step=1,\n",
        "                label=\"Chat Font Size (px)\",\n",
        "                info=\"Message text in chat bubbles\",\n",
        "            )\n",
        "\n",
        "            ui_scale = gr.Slider(\n",
        "                80, 150, DEFAULT_UI_SCALE, step=5,\n",
        "                label=\"UI Scale (%)\",\n",
        "                info=\"Buttons, labels, inputs, sidebar â€” everything else\",\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"**Presets**\")\n",
        "            with gr.Row():\n",
        "                preset_general  = gr.Button(\"ğŸ“ General\", size=\"sm\")\n",
        "                preset_tool     = gr.Button(\"ğŸ”§ Tool\", size=\"sm\")\n",
        "                preset_creative = gr.Button(\"ğŸ¨ Creative\", size=\"sm\")\n",
        "\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # Event Wiring\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "    # â”€â”€ Sidebar collapse / expand â”€â”€\n",
        "    def do_collapse():\n",
        "        return False, gr.update(visible=False), gr.update(visible=True)\n",
        "\n",
        "    def do_expand():\n",
        "        return True, gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    collapse_btn.click(\n",
        "        do_collapse, None,\n",
        "        [sidebar_state, sidebar_panel, sidebar_rail],\n",
        "    )\n",
        "\n",
        "    rail_expand_btn.click(\n",
        "        do_expand, None,\n",
        "        [sidebar_state, sidebar_panel, sidebar_rail],\n",
        "    )\n",
        "\n",
        "    rail_newchat_btn.click(\n",
        "        on_new_chat,\n",
        "        [chat_store, active_chat_id, chatbot],\n",
        "        [chat_store, active_chat_id, chatbot, chat_list, rename_input],\n",
        "    ).then(\n",
        "        do_expand, None,\n",
        "        [sidebar_state, sidebar_panel, sidebar_rail],\n",
        "    )\n",
        "\n",
        "    # â”€â”€ Settings toggle â”€â”€\n",
        "    def toggle_settings(s):\n",
        "        return not s, gr.update(visible=not s)\n",
        "\n",
        "    settings_toggle.click(\n",
        "        toggle_settings, [settings_state], [settings_state, settings_panel]\n",
        "    )\n",
        "\n",
        "    # â”€â”€ Upload toggle â”€â”€\n",
        "    upload_visible_state = gr.State(False)\n",
        "\n",
        "    def toggle_upload(visible):\n",
        "        return gr.update(visible=not visible), not visible\n",
        "\n",
        "    upload_btn.click(\n",
        "        toggle_upload,\n",
        "        [upload_visible_state],\n",
        "        [file_upload, upload_visible_state],\n",
        "    )\n",
        "\n",
        "    def on_file_selected(file_obj):\n",
        "        if file_obj is None:\n",
        "            return gr.update(visible=False, value=\"\")\n",
        "        fname = os.path.basename(file_obj.name) if hasattr(file_obj, 'name') else \"file\"\n",
        "        ext = os.path.splitext(fname)[1].lower()\n",
        "        if ext in SUPPORTED_IMG_EXT:\n",
        "            return gr.update(visible=True, value=f\"ğŸ–¼ï¸ **{fname}** attached (âš ï¸ model is text-only)\")\n",
        "        elif ext in SUPPORTED_TEXT_EXT or ext == \".pdf\" or ext == \".csv\":\n",
        "            fsize = os.path.getsize(file_obj.name) / 1024\n",
        "            return gr.update(visible=True, value=f\"ğŸ“ **{fname}** ({fsize:.0f} KB) â€” will be sent with next message\")\n",
        "        else:\n",
        "            return gr.update(visible=True, value=f\"ğŸ“„ **{fname}** â€” unsupported format\")\n",
        "\n",
        "    file_upload.change(on_file_selected, [file_upload], [upload_status])\n",
        "\n",
        "    # â”€â”€ Sidebar events â”€â”€\n",
        "    search_box.change(on_search, [search_box, chat_store], [chat_list])\n",
        "\n",
        "    new_chat_btn.click(\n",
        "        on_new_chat,\n",
        "        [chat_store, active_chat_id, chatbot],\n",
        "        [chat_store, active_chat_id, chatbot, chat_list, rename_input],\n",
        "    )\n",
        "\n",
        "    chat_list.change(\n",
        "        on_select_chat,\n",
        "        [chat_list, chat_store, active_chat_id, chatbot],\n",
        "        [active_chat_id, chatbot, chat_store, rename_input],\n",
        "    )\n",
        "\n",
        "    delete_btn.click(\n",
        "        on_delete,\n",
        "        [chat_list, chat_store, active_chat_id, chatbot],\n",
        "        [chat_store, active_chat_id, chatbot, chat_list, rename_input],\n",
        "    )\n",
        "\n",
        "    rename_btn.click(\n",
        "        on_rename,\n",
        "        [chat_list, rename_input, chat_store],\n",
        "        [chat_store, chat_list, rename_input],\n",
        "    )\n",
        "\n",
        "    dl_txt_btn.click(on_download_txt, [chat_list, chat_store], [download_output])\n",
        "    dl_json_btn.click(on_download_json, [chat_list, chat_store], [download_output])\n",
        "\n",
        "    # â”€â”€ Context reload â”€â”€\n",
        "    ctx_reload_btn.click(reload_context, [ctx_dropdown], [ctx_status])\n",
        "\n",
        "    # â”€â”€ Settings: font + UI scale â”€â”€\n",
        "    font_size.change(on_font_change, [font_size, ui_scale], [font_css])\n",
        "    ui_scale.change(on_font_change, [font_size, ui_scale], [font_css])\n",
        "\n",
        "    # â”€â”€ Settings: presets â”€â”€\n",
        "    def p_general():  return 1.0, 1024, 0.95, 0.01, 1.0\n",
        "    def p_tool():     return 0.7, 1024, 1.0,  0.01, 1.0\n",
        "    def p_creative(): return 1.2, 2048, 0.95, 0.01, 1.0\n",
        "\n",
        "    preset_out = [temperature, max_tokens, top_p, min_p, repeat_penalty]\n",
        "    preset_general.click(p_general,   None, preset_out)\n",
        "    preset_tool.click(p_tool,         None, preset_out)\n",
        "    preset_creative.click(p_creative, None, preset_out)\n",
        "\n",
        "    # â”€â”€ Clear current chat â”€â”€\n",
        "    def clear_current(store, active):\n",
        "        if active in store:\n",
        "            store[active][\"history\"] = []\n",
        "        return [], store\n",
        "\n",
        "    clear_btn.click(\n",
        "        clear_current,\n",
        "        [chat_store, active_chat_id],\n",
        "        [chatbot, chat_store],\n",
        "    )\n",
        "\n",
        "    # â”€â”€ Chat submit â”€â”€\n",
        "    chat_inputs = [\n",
        "        txt, file_upload, chatbot, chat_store, active_chat_id,\n",
        "        system_prompt, max_tokens, temperature, top_p, min_p, repeat_penalty,\n",
        "    ]\n",
        "    chat_outputs = [txt, file_upload, chatbot, chat_store, chat_list]\n",
        "\n",
        "    txt.submit(chat_fn, chat_inputs, chat_outputs)\n",
        "    send_btn.click(chat_fn, chat_inputs, chat_outputs)\n",
        "\n",
        "demo.launch(debug=False, share=True, quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c8ClArO6_syq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# @title ğŸ§¹ CELL 5 â€” Full Unload (Model + Gradio + VRAM + RAM)\n",
        "# ============================================================\n",
        "\n",
        "import gc\n",
        "\n",
        "# â”€â”€ Stop Gradio â”€â”€\n",
        "try:\n",
        "    demo.close()\n",
        "    print(\"âœ… Gradio stopped\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ Gradio not running\")\n",
        "\n",
        "# â”€â”€ Unload model â”€â”€\n",
        "try:\n",
        "    llm.close()\n",
        "    print(\"âœ… llm.close() done\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del llm\n",
        "    print(\"âœ… llm deleted\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ llm not found\")\n",
        "\n",
        "# â”€â”€ Force garbage collection (multiple passes) â”€â”€\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "\n",
        "# â”€â”€ Clear CUDA cache â”€â”€\n",
        "try:\n",
        "    import torch\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    print(\"âœ… CUDA cache cleared\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# â”€â”€ Verify â”€â”€\n",
        "import psutil, subprocess\n",
        "\n",
        "ram_used = psutil.virtual_memory().used / 1e9\n",
        "try:\n",
        "    smi = subprocess.check_output(\n",
        "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.free\",\n",
        "         \"--format=csv,noheader,nounits\"], text=True\n",
        "    ).strip().split(\", \")\n",
        "    print(f\"\\nğŸ“Š VRAM: {int(smi[0])} MiB used / {int(smi[1])} MiB free\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(f\"ğŸ“Š RAM:  {ram_used:.1f} GB used\")\n",
        "print(\"\\nğŸ§¹ Cleanup complete â€” safe to reload or switch models\")\n",
        "print(\"   Run Cell 2 â†’ Cell 3 â†’ Cell 4 to load a different model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}